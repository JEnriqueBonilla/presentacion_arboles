---
title: "Árboles de Clasificación"
subtitle: "Introducción al Aprendizaje Estadístico"
author: "Kael Huerta, Enrique Bonilla"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
header-includes: 
- \usepackage[spanish]{babel}
- \usepackage[utf8]{inputenc}
- \usepackage{amsmath,amsfonts,amsthm}
- \usepackage{graphicx}
bibliography: bibliografia.bib
link-citations: yes
---

```{r setup, include = F}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

### Nota: los paquetes que se utilizaran en estas notas son los siguientes:
```{r,echo = T, warning = F, message = F,error = F}
require(ggplot2)
require(stringr)
require(lubridate)
require(plyr)
require(reshape2)
require(dplyr)
require(rpart)
require(randomForest)
require(gbm)
require(partykit)
```


# Introducción 


##  Aprendizaje Estadístico

En el mundo del análisis de datos, existen 2 formas de concebir el 
modelado estadístico:
    
- **Data modeling culture**: su objetivo es el diagnóstico.
Fuerte en teoría y utiliza pocos datos.

- **Alogiritmic modeling culture**: su finalidad es predecir
futuras observaciones. Aunque no se ha desarrollado
mucha teoría matemática, se desempeña muy bien ante 
grandes cantidades de datos.

Cada uno de estos enfoques es útil en distintas situaciones. 
En particular, el Aprendizaje Estadístico utiliza el segundo enfoque. 
    
Algunos términos relacionados que son frecuentemente mencionados en
la literatura son los siguientes

- **Aprendizaje de Máquina**: análisis de algoritmos;
- **Minería de Datos**: bases de datos grandes;
- **Aprendizaje estadístico**: teoría estadística detrás de
las últimas dos.

El aprendizaje estadístico se divide en dos principales ramas:
   
- **Aprendizaje Supervisado**: predecir variable dada $Y$ 
    (conocemos respuesta). Si la variable respuesta es
    numérica, se conoce como problema de *regresión*. 
    Si es categórica, se denomina problema de *clasificación*.




```{r fig-two-separate, fig.cap=sprintf("Problema de %s.", c("regresión", "clasificación")), cache = F}

#Método: regresión lineal
#Datos: old faithful géiser en Yellowstone National Park, Wyoming, USA
 
faithful %>% ggplot(aes(x = eruptions, y = waiting)) + 
  geom_point() +
  geom_smooth(method='lm') +
  xlab("Duración de erupción") + 
  ylab("Tiempo de espera para siguiente erupción")

#Método: regresión lineal
#Datos: flores de Fisher

datos <- iris %>% 
  filter(Species %in% c("setosa", "versicolor") ) %>% 
  mutate(specie = ifelse(Species== "setosa", 0, 1)) %>% 
  mutate(specie = as.numeric(specie))
  
modelo <- datos %>% 
  lm(formula = "specie ~ Sepal.Length + Sepal.Width")

# Si y > .5 entonces se clasifica 1

 datos %>% 
   ggplot(aes(x = Sepal.Length, y = Sepal.Width, 
              colour = Species)) +
   geom_point() + 
   geom_abline(intercept = -1.28, slope = .79)  + 
   annotate("text", x = 6, y = 4, 
            label = ".50 = -.28 + .47x1 - .59x2") + 
   xlab("Longitud del sépalo") + 
   ylab("Anchura del sépalo")


```

- **Aprendizaje No Supervisado**: no hay variable respuesta.
Se trata de encontrar estructuras intrínsecas en los datos.

```{r fig-main, fig.cap = "Problema de agrupamiento.", cache = F}
 
#Método: k-medias
#Datos: old faithful géiser en Yellowstone National Park, Wyoming, USA

modelo <- kmeans(faithful, 2)

datos <- data.frame(faithful, 
                   grupo = as.character(modelo$cluster))

datos %>% 
  ggplot(aes(x = eruptions, y = waiting, color = grupo)) + 
  geom_point() +
  xlab("Duración de erupción") + 
  ylab("Tiempo de espera para siguiente erupción")
```

Los *árboles de clasificación* (CART, Bosques Aleatorios, etc.)
métodos de aprendizaje supervisado. Estos pueden tener como
respuesta una variable numérica ($Y$) o una variable 
categórica ($G$).

## Supuestos y notación

Se define a $X = (X_1, \ldots, X_p)$ como la variable de entrada 
y a $Y$ como la variable respuesta, cuantitativa ($G$ cualitativa).
Se asume que $X,Y$ están relacionadas a través de un modelo 
probabilístico $P(X,Y)$ donde $P(X,Y)$ es su distribución de 
probabilidad conjunta.

En el caso de regresión, suponemos que se puede escribir $Y = f(X) + \epsilon$
donde $E(\epsilon) = 0$ y es independiente de X. Para este modelo, 
 $f(x) = E(Y \mid X = x)$.^[En algunos problemas,
se puede suponer que $Y = f(x)$ o que los errores no son independientes
e idénticamente distribuidos.] Nuestro objetivo es estimar $f(x)$
por medio de una aproximación $\hat{f}$ usando los datos observados.
^[Típicamente, la $f$ estimada depende de un parámetro
o conjunto de parámetros de ajuste $\alpha$, i.e, $\hat{f}_{\alpha}$. ]

Cuando se trata del problema de clasificación, nuestro objetivo es 
estimar $P(G \mid X)$ y esto se modela directamente. 

Para construir la regla de predicción ($\hat{f}$ o $\hat{P}$ 
dependiendo el caso), usamos datos observados 
$\Gamma  = \{(x_1, y_1), \ldots, (x_n, y_n)\}$. A este conjunto, se le conoce 
como conjunto de entrenamiento. 

## Teoría de decisión


