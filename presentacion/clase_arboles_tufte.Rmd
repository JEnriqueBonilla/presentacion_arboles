---
title: "Árboles de Clasificación"
subtitle: "Introducción al Aprendizaje Estadístico"
author: "Kael Huerta, Enrique Bonilla"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
header-includes: 
- \usepackage[spanish]{babel}
- \usepackage[utf8]{inputenc}
- \usepackage{amsmath,amsfonts,amsthm}
- \usepackage{graphicx}
bibliography: bibliografia.bib
link-citations: yes
nocite: | 
  @notas
---

---

```{r setup, include = F}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```


### Nota: los paquetes que se utilizaran en estas notas son los siguientes:
```{r,echo = T, warning = F, message = F,error = F}
require(ggplot2)
require(ggdendro)
require(plyr)
require(dplyr)
require(tidyr)
require(kknn)
require(rpart)
require(tree)
require(mlbench)
require(randomForest)
require(gbm)
```

```{r, echo = F}
## Cambiando el tema de las gráficas
theme_set(theme_bw())
```

# Introducción 


## Aprendizaje Estadístico

En el mundo del análisis de datos existen 2 formas de concebir el 
modelado estadístico:
    
- **Data modeling culture**: su objetivo es el diagnóstico.
Fuerte en teoría y utiliza pocos datos.

- **Algorithmic modeling culture**: su finalidad es predecir
futuras observaciones. Aunque no se ha desarrollado
mucha teoría matemática, se desempeña muy bien ante 
grandes cantidades de datos.

Cada uno de estos enfoques es útil en distintas situaciones. 
En particular, el Aprendizaje Estadístico utiliza el segundo enfoque. 
    
Algunos términos relacionados que son frecuentemente mencionados en
la literatura son los siguientes:

- **Aprendizaje de Máquina**: análisis de algoritmos.
- **Minería de Datos**: bases de datos grandes.
- **Aprendizaje estadístico**: teoría estadística detrás de
las últimas dos.

El aprendizaje estadístico se divide en dos principales ramas:
   
- **Aprendizaje Supervisado**: predecir la variable dada $Y$ 
    (conocemos la respuesta). Si la variable de respuesta es
    cuantitativa se conoce como problema de *regresión*. 
    Si es cualitativa, se denomina problema de *clasificación*.
- **Aprendizaje No Supervisado**: no hay variable respuesta.
Se trata de encontrar estructuras intrínsecas en los datos.

```{r fig-two-separate, fig.cap=sprintf("Aprendizaje supervisado. Problema de %s.", c("regresión", "clasificación")), cache = F}

## Método: regresión lineal
## Datos: old faithful géiser en Yellowstone National Park, Wyoming, USA
 
faithful %>%
  ggplot(aes(x = eruptions, y = waiting)) + 
    geom_point() +
    geom_smooth(method='lm') +
    xlab("Duración de erupción (min.)") + 
    ylab("Tiempo de espera para siguiente erupción (min.)")

## Método: regresión lineal
## Datos: flores de Fisher
## Librerias: base

datos <- iris %>% 
  filter(Species %in% c("setosa", "versicolor") ) %>% 
  mutate(specie = as.numeric(ifelse(Species== "setosa", 0, 1)))
  
modelo <- datos %>% 
  lm(formula = "specie ~ Sepal.Length + Sepal.Width")

## Si y > .5 entonces se clasifica 1

datos %>%
  ggplot(aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +
    geom_point() + 
    geom_abline(intercept = -1.28, slope = .79)  + 
    annotate("text", x = 6, y = 4, label = ".50 = -.28 + .47x1 - .59x2") +
    xlab("Longitud del sépalo") +
    ylab("Anchura del sépalo")
```


```{r fig-main, fig.cap = "Aprendizaje no supervisado. Problema de agrupamiento.", cache = F}
 
## Método: k-medias
## Datos: old faithful géiser en Yellowstone National Park, Wyoming, USA
## Librerias: base

modelo <- kmeans(faithful, 2)

datos <- data.frame(faithful, grupo = as.character(modelo$cluster))

datos %>% 
  ggplot(aes(x = eruptions, y = waiting, color = grupo)) + 
  geom_point() +
  xlab("Duración de erupción") + 
  ylab("Tiempo de espera para siguiente erupción")
```

Los *árboles de clasificación* (CART, Bosques Aleatorios, etc.)
son métodos de aprendizaje supervisado. Estos pueden tener como
respuesta una variable numérica ($Y$) o una variable 
categórica ($G$).

## Supuestos y notación

Se define a $X = (X_1, \ldots, X_p)$ como la variable de entrada 
y a $Y$ como la variable de respuesta cuantitativa, $G$ en caso de ser
cualitativa. Se asume que $X$ y $Y$ están relacionadas a través de un modelo
probabilístico $P(X,Y)$ donde $P(X,Y)$ es su distribución de
probabilidad conjunta. Por lo general, $P(X,Y)$ es un objeto
complicado que no conocemos y es difícil de estimar.

En el caso de regresión, suponemos que se puede escribir $Y = f(X) + \epsilon$
donde $E[\epsilon] = 0$ y es independiente de X.^[La idea es que 
$f(X)$ captura toda la información sistemática de $X$ acerca de 
$Y$, así que $\epsilon$ es independiente de $X$.] Para este 
modelo, $f(x) = E[Y \mid X = x]$.^[En algunos problemas,
se puede suponer que $Y = f(X)$ o que los errores no son independientes
e idénticamente distribuidos.] Nuestro objetivo es estimar $f(X)$
por medio de una aproximación $\hat{f}$ usando los datos observados.
^[Típicamente, la $f$ estimada depende de un parámetro
o conjunto de parámetros de **complejidad** $\alpha$, i.e, $\hat{f}_{\alpha}$.]

Cuando se trata del problema de clasificación, nuestro objetivo es 
estimar $P(G \mid X)$ y esto se modela directamente. 

Para construir la regla de predicción ($\hat{f}$ o $\hat{P}$ 
dependiendo el caso) usamos datos observados. 
Al conjunto de datos observados $\Gamma  = \{(x_1, y_1), \ldots, (x_n, y_n)\}$ 
con los que se evalúa el modelo se le conoce como
*conjunto de entrenamiento*. 

## Teoría de decisión

Al igual que en la estadística bayesiana, el aprendizaje estadístico
se apoya de la teoría de decisión para tomar decisiones basado
en los principios de la elección coherente. Denotaremos 
a la *función de pérdida* como $L(Y,\hat{f}(X))$. En teoría, nuestro objetivo 
será encontrar $\hat{f}(X)$ dentro de una familia de funciones $F$^[Este 
problema se puede traducir en encontrar $\alpha$.] que minimice  
la *pérdida esperada* sobre observaciones futuras^[El planteamiento 
común en teoría de decisión es maximizar el valor esperado de una 
función de utilidad $U(X)$. Se puede ver que estos planteamientos 
son equivales si $U(X)= -\hat{f}(X)$.], i.e,
$$\begin{equation*}
\begin{aligned}
& \underset{f}{\text{min}}
& & E[L(Y_0,\hat{f}(X_0)] \\
& \text{s.a.} & & f\in F \\
& & & X_0,Y_0 \in P(X,Y)
\end{aligned}
\end{equation*}$$ 
En la práctica, esto se traduce a minimizar el error de predicción
de nuestro modelo condicionado a los datos de entrenamiento.


Las funciones de pérdida más comunes en la literatura son las siguientes:

- **Problema de regresión**:  $$L(Y,\hat{f}(X)) =
  \begin{cases}
    (Y - \hat{f}(X))^2       & \quad \text{pérdida cuadrática}\\
    \mid X-\hat{f}(X) \mid   & \quad  \text{pérdida absoluta}\\
  \end{cases}
$$

- **Problema de clasificación**:  
$$L(G,\hat{G}(X)) =  G \neq \hat{G} \quad \text{pérdida 0-1}$$

$$L(G, \hat{P}(G \mid X)) = -2\sum\limits_{k=1}^K I(G = k)log(\hat{P}(G = k \mid X)) \quad \text{log verosimilitud o devianza}$$

Como primera opción, uno podría pensar que una estimación
del error de predicción se puede obtener por medio del *error de entrenamiento* 
$$\bar{err} = \sum\limits_{i=1}^n L(y_i, \hat{f}(x_i))$$
con $(x_i,y_i) \in \Gamma$. Sin embargo, el error de entrenamiento
subestima, generalmente, esta cantidad. Esto es porque el error
de entrenamiento decrece constantemente al aumentar la complejidad
del modelo, típicamente llegando a cero, pues si el modelo 
es muy complejo captura ruido de $\epsilon$.

La manera correcta de estimar el error que vamos a tener 
al predecir con $\hat{f}(X)$ al ser entrenado con una muestra
$\gamma$ es estimando el *error de prueba* también conocido como
*error de predición* o *generalization error*
$$E_{rr\gamma} = E[L(Y,\hat{f}(X)) \mid \gamma]$$
donde $\gamma$ es una muestra de $P(X,Y)$.^[observemos 
que $(X,Y)$ son elementos de $P(X,Y)$ no de $\Gamma$ 
necesariamente.] 

Como se mencionó anteriormente, estimar $E_{rr\gamma}$ 
es nuestro principal objetivo pues nos revela no sólo como se 
comporta la predicción de nuestro modelo $\hat{f}(X)$, si no
que, además, sabemos como se va a comportar al ser entrenado
con el conjunto de entrenamiento $\Gamma$ que poseemos. Sin embargo, 
parece que no es posible estimar este error condicional de forma 
efectiva solo con el conjunto de entrenamiento.
<!-- Explicar brevemente por qué -->

Lo que se puede hacer es estimar el 
*error de predicción esperado* o *error de prueba esperado*^[
$E_{rr}=E_\gamma[E_{X,Y}[L(Y,\hat{F}(C)) \mid \gamma]]$
]
$$E[L(Y,\hat{f}(X))] = E_{rr} = E[E_{rr\gamma}]$$
y usarlo para comparar y seleccionar modelos.
Este valor es más amable al análisis estadístico además
de que puede ser estimado con sólo el uso de la muestra
de entrenamiento.
<!-- Con la muestra de entrenamiento no, con la de prueba ¿o no?.
Según yo con un subconjunto de los datos observados -->

El error de predicción esperado puede ser aproximado analíticamente
por métodos *in-sample* (AIC, DIC, MDL o SRR)^[En 
realidad, algunos de métodos estiman $E_y[E_{inn}]$. Para más 
información ver la sección 7.4 de @friedman2001elements .] 
o por métodos de reuso de muestras (validación cruzada, *bootstrap*).
<!-- ¿Por qué aparece una subnota a la subnota? -->


Si tenemos muchos datos, lo mejor que podemos hacer es dividir
la muestra observada en 3 conjuntos ajenos:

- **Conjunto de entrenamiento**: se usa para entrenar el modelo 
(puede sobre-ajustar). Se recomienda, empíricamente, $50\%$ de
la muestra.
<!-- Yo te diría que de 50% a 80% dependiendo de la cantidad de datos
Aunque eso puede ser una nota-->

- **Conjunto de prueba**: se usa para seleccionar modelos 
(aún si estima $E_{rr\gamma}$, se puede sobre-ajustar). 
Se recomienda, empíricamente, $25\%$ de
la muestra.

- **Conjunto de validación**: se utiliza para aproximar $E_{rr\gamma}$
de nuestro modelo final. Se recomienda, empíricamente, $25\%$ de
la muestra y sólo debe ser utilizado al final para calcular el error. 
**No** debe formar parte del proceso de selección de modelos (puede
sobre-ajustar).
<!-- Además de sobreajuste, estarías dando una sub-estimación del valor
real de error -->

<!-- VER COMENTARIO SIGUIENTE: La nota según yo podría ser algo así:

[Cuando se cuenta con una muestra de datos observados muy
pequeña se puede utilizar hasta 80% para entramiento y prueba con validación
cruzada y validación el 20% restante]

Pero no la agregué porque creo que no mencionas validación cruzada, entonces
tómalo como sugerencia únicamente -->

Si tenemos un menor tamaño de muestra, ésta se separa sólo en
conjuntos ajenos de entrenamiento y validación. Con el conjunto
de entrenamiento estimamos $E_{rr}$ (por medio de métodos de
reuso de muestra o *in-sample*) y comparamos / seleccionamos modelos.
Con el conjunto de validación estimamos $E_{rr\gamma}$ del modelo final. 

<!-- Olvida mi nota anterior, con esto basta. Quizá añadiría el dividir en 80%
y 20% o algo por el estilo -->

## Descomposición en varianza-sesgo

Si asumimos que estamos en el caso de regresión 
($Y = F(X) + \epsilon$, $E[\epsilon]=0$, $Var(\epsilon)= \sigma_\epsilon^2$)
y usamos la pérdida cuadrática se puede demostrar que ^[Por simplicidad,
supondremos que sólo nos interesa hacer predicción para $X = x_0$]
$$E_{rr}(x_0) = \sigma_\epsilon^2 + sesgo_\gamma^2(x_0) + Var_\gamma(x_0)$$

donde

- $\sigma_\epsilon^2 = Var(Y \mid X=x_0)$ es el error irreducible. 
No depende del ajuste, es aleatoriedad del fenómeno.

- $sesgo_\gamma^2(x_0) = (E[Y \mid X = x_0] -E_\gamma[\hat{f}_\gamma(x_0)])^2$ 
es el sesgo del método. Mide en promedio cuanto se desvía 
$\hat{f}_\gamma(x_o)$ del óptimo $E[y \mid X = x_0]$. Entre menos complejo sea
el método aumenta el sesgo y, por lo tanto, aumenta el error.

- $Var_\gamma(x_0) = E_\gamma[(\hat{f}_\gamma(x_0)-E_\gamma[\hat{f}_\gamma(x_0)])^2]$
es la varianza del método. mide qué tanto varían las predicciones
entre muestras de entrenamiento. Un método muy complejo es
un método inestable en el que las predicciones varían mucho
entres muestras de entrenamiento. 

Esta descomposición se encuentra, generalmente, independientemente 
de la función de pérdida y del tipo de problema. Por lo general,
$E_{rr\gamma}$ tiene un comportamiento con la misma estructura que 
$E_{rr}$ pero no igual.

<!-- Valdría la pena hablar un poco más del "trade-off" entre complejidad y
sesgo -->


```{r , fig.cap = "Varianza-Sesgo: entrenamiento vs prueba.", cache = F}
 
## Método: k vecinos más cercanos
## Datos: Munich Rent Standards 
## Librerias: kknn

data(miete)

tamano <- floor(0.65 * nrow(miete))

set.seed(203040)

entrena.ind <- sample(seq_len(nrow(miete)), 
                      size = tamano)

entrena <- miete[entrena.ind, ]

prueba <- miete[-entrena.ind, ]


errores.vmc <- ldply(c(1, 5, 10, 20, 50, 100, 250, 500), 
                     function(i) {
  
    vecino.k.prueba <- kknn(nmqm ~ wfl + bjkat + zh, 
                            train = entrena, 
                            test = prueba, 
                            k = i, kernel = "rectangular")
    
    vecino.k.entrena <- kknn(nmqm ~ wfl + bjkat + zh, 
                             train = entrena, 
                             test = entrena, 
                             k = i, kernel = "rectangular")
    
    error.prueba <- mean((fitted(vecino.k.prueba) - 
                            prueba$nmqm)^2)
    error.entrena <- mean((fitted(vecino.k.entrena) - 
                             entrena$nmqm)^2)
    
    data.frame(k = i, prueba = error.prueba, 
               entrenamiento = error.entrena)
})

err.m <- errores.vmc %>% 
  gather(variable, valor, -k) %>% 
  mutate(n.k = floor(500/k) )
  
err.m %>% 
  ggplot(aes(x = factor(n.k), y = valor, 
             colour = variable,
             group = variable)) + 
  geom_line() + 
  geom_point() + 
  xlab("Grados de libertad") + 
  ylab("Error")
```

# Árboles de clasificación

# Referencias



