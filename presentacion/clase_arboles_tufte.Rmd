---
title: "Árboles de Clasificación y regresión"
subtitle: "Introducción al Aprendizaje Estadístico"
author: "Kael Huerta, Enrique Bonilla"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
header-includes: 
- \usepackage[spanish]{babel}
- \usepackage[utf8]{inputenc}
- \usepackage{amsmath,amsfonts,amsthm}
- \usepackage{graphicx}
bibliography: bibliografia.bib
link-citations: yes
nocite: | 
  @notas
---

---

```{r setup, include = F}
library(tufte)
## invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```


### Nota: los paquetes que se utilizaran en estas notas son los siguientes:
```{r,echo = T, warning = F, message = F,error = F}
require(ggplot2)
require(ggdendro)
require(plyr)
require(dplyr)
require(tidyr)
require(kknn)
require(rpart)
require(tree)
require(mlbench)
require(randomForest)
require(gbm)
require(data.table)
```

```{r, echo = F}
## Cambiando el tema de las gráficas
theme_set(theme_bw())
```

# Introducción 


## Aprendizaje Estadístico

En el mundo del análisis de datos existen 2 formas de concebir el 
modelado estadístico:
    
- **Data modeling culture**: su objetivo es el diagnóstico.
Fuerte en teoría y utiliza pocos datos.

- **Algorithmic modeling culture**: su finalidad es predecir
futuras observaciones. Aunque no se ha desarrollado
mucha teoría matemática, se desempeña muy bien ante 
grandes cantidades de datos.

Cada uno de estos enfoques es útil en distintas situaciones. 
En particular, el Aprendizaje Estadístico utiliza el segundo enfoque. 
    
Algunos términos relacionados que son frecuentemente mencionados en
la literatura son los siguientes:

- **Aprendizaje de Máquina**: análisis de algoritmos.
- **Minería de Datos**: bases de datos grandes.
- **Aprendizaje estadístico**: teoría estadística detrás de
las últimas dos.

El aprendizaje estadístico se divide en dos principales ramas:
   
- **Aprendizaje Supervisado**: predecir la variable dada $Y$ 
    (conocemos la respuesta). Si la variable de respuesta es
    cuantitativa se conoce como problema de *regresión*. 
    Si es cualitativa, se denomina problema de *clasificación*.
- **Aprendizaje No Supervisado**: no hay variable respuesta.
Se trata de encontrar estructuras intrínsecas en los datos.

```{r fig-two-separate, fig.cap=sprintf("Aprendizaje supervisado. Problema de %s.", c("regresión", "clasificación"))}

## Método: regresión lineal
## Datos: old faithful géiser en Yellowstone National Park, Wyoming, USA
## Librerias: base
 
faithful %>%
  ggplot(aes(x = eruptions, y = waiting)) + 
    geom_point() +
    geom_smooth(method='lm') +
    xlab("Duración de erupción (min.)") + 
    ylab("Tiempo de espera para siguiente erupción (min.)")

## Método: regresión lineal
## Datos: flores de Fisher
## Librerias: base

datos <- iris %>% 
  filter(Species %in% c("setosa", "versicolor") ) %>% 
  mutate(specie = as.numeric(ifelse(Species== "setosa", 0, 1)))
  
modelo <- datos %>% 
  lm(formula = "specie ~ Sepal.Length + Sepal.Width")

## Si y > .5 entonces se clasifica 1

datos %>%
  ggplot(aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) +
    geom_point() + 
    geom_abline(intercept = -1.28, slope = .79)  + 
    annotate("text", x = 6, y = 4, label = ".50 = -.28 + .47x1 - .59x2") +
    xlab("Longitud del sépalo") +
    ylab("Anchura del sépalo")
```


```{r fig-main, fig.cap = "Aprendizaje no supervisado. Problema de agrupamiento."}
 
## Método: k-medias
## Datos: old faithful géiser en Yellowstone National Park, Wyoming, USA
## Librerias: base

modelo <- kmeans(faithful, 2)

datos <- data.frame(faithful, grupo = as.character(modelo$cluster))

datos %>% 
  ggplot(aes(x = eruptions, y = waiting, color = grupo)) + 
  geom_point() +
  xlab("Duración de erupción") + 
  ylab("Tiempo de espera para siguiente erupción")
```

Los *métodos basados en árboles* (CART, Bosques Aleatorios, etc.)
son algoritmos de aprendizaje supervisado. Estos pueden tener como
respuesta una variable numérica ($Y$) o una variable 
categórica ($G$).

## Supuestos y notación

Se define a $X = (X_1, \ldots, X_p)$ como la variable de entrada 
y a $Y$ como la variable de respuesta cuantitativa, $G$ en caso de ser
cualitativa. Se asume que $X$ y $Y$ están relacionadas a través de un modelo
probabilístico $P(X,Y)$ donde $P(X,Y)$ es su distribución de
probabilidad conjunta. Por lo general, $P(X,Y)$ es un objeto
complicado que no conocemos y es difícil de estimar.

En el caso de regresión, suponemos que se puede escribir $Y = f(X) + \epsilon$
donde $E[\epsilon] = 0$ y es independiente de X.^[La idea es que 
$f(X)$ captura toda la información sistemática de $X$ acerca de 
$Y$, así que $\epsilon$ es independiente de $X$.] Para este 
modelo, $f(x) = E[Y \mid X = x]$.^[En algunos problemas,
se puede suponer que $Y = f(X)$ o que los errores no son independientes
e idénticamente distribuidos.] Nuestro objetivo es estimar $f(X)$
por medio de una aproximación $\hat{f}$ usando los datos observados.
^[Típicamente, la $f$ estimada depende de un parámetro
o conjunto de parámetros de **ajuste** $\alpha$, i.e, $\hat{f}_{\alpha}$.]

Cuando se trata del problema de clasificación, nuestro objetivo es 
estimar $P(G \mid X)$ y esto se modela directamente. 

Para construir la regla de predicción ($\hat{f}$ o $\hat{P}$ 
dependiendo el caso) usamos datos observados. 
Al conjunto de datos observados $\Gamma  = \{(x_1, y_1), \ldots, (x_n, y_n)\}$ 
con los que se evalúa el modelo se le conoce como
*conjunto de entrenamiento*. 

## Teoría de decisión

Al igual que en la estadística bayesiana, el aprendizaje estadístico
se apoya de la teoría de decisión para tomar decisiones basado
en los principios de la elección coherente. Denotaremos 
a la *función de pérdida* como $L(Y,\hat{f}(X))$. En teoría, nuestro objetivo 
será encontrar $\hat{f}(X)$ dentro de una familia de funciones $F$^[Este 
problema se puede traducir en encontrar $\alpha$.] que minimice  
la *pérdida esperada* sobre observaciones futuras^[El planteamiento 
común en teoría de decisión es maximizar el valor esperado de una 
función de utilidad $U(X)$. Se puede ver que estos planteamientos 
son equivales si $U(X)= -\hat{f}(X)$.], i.e,
$$\begin{equation*}
\begin{aligned}
& \underset{f}{\text{min}}
& & E[L(Y_0,\hat{f}(X_0)] \\
& \text{s.a.} & & f\in F \\
& & & X_0,Y_0 \in P(X,Y)
\end{aligned}
\end{equation*}$$ 

En la práctica, esto se traduce a minimizar el error de predicción
de nuestro modelo condicionado a los datos de entrenamiento.


Las funciones de pérdida más comunes en la literatura son las siguientes:

- **Problema de regresión**:  $$L(Y,\hat{f}(X)) =
  \begin{cases}
    (Y - \hat{f}(X))^2       & \quad \text{pérdida cuadrática}\\
    \mid X-\hat{f}(X) \mid   & \quad  \text{pérdida absoluta}\\
  \end{cases}
$$

- **Problema de clasificación**:  
$$L(G,\hat{G}(X)) =  G \neq \hat{G} \quad \text{pérdida 0-1}$$

$$L(G, \hat{P}(G \mid X)) = -2\sum\limits_{k=1}^K I(G = k)log(\hat{P}(G = k \mid X)) \quad \text{log verosimilitud o devianza}$$

Como primera opción, uno podría pensar que una estimación
del error de predicción se puede obtener por medio del *error de entrenamiento* 
$$\bar{err} = \sum\limits_{i=1}^n L(y_i, \hat{f}(x_i))$$
con $(x_i,y_i) \in \Gamma$. Sin embargo, el error de entrenamiento
subestima, generalmente, esta cantidad. Esto es porque el error
de entrenamiento decrece constantemente al aumentar la **complejidad**
del modelo, típicamente llegando a cero, pues si el modelo 
es muy complejo captura ruido de $\epsilon$.

La manera correcta de estimar el error que vamos a tener 
al predecir con $\hat{f}(X)$ al ser entrenado con una muestra
$\gamma$ es estimando el *error de prueba* también conocido como
*error de predición* o *generalization error*
$$E_{rr\gamma} = E[L(Y,\hat{f}(X)) \mid \gamma]$$
donde $\gamma$ es una muestra de $P(X,Y)$.^[observemos 
que $(X,Y)$ son elementos de $P(X,Y)$ no de $\Gamma$ 
necesariamente.] 

Como se mencionó anteriormente, estimar $E_{rr\gamma}$ 
es nuestro principal objetivo pues nos revela no sólo como se 
comporta la predicción de nuestro modelo $\hat{f}(X)$, si no
que, además, sabemos como se va a comportar al ser entrenado
con el conjunto de entrenamiento $\Gamma$ que poseemos. Sin embargo, 
parece que no es posible estimar este error condicional de forma 
efectiva **_sólo con el conjunto de entrenamiento_**. Se
necesitarían datos independientes a esta muestra para hacerlo.


Lo que se puede hacer es estimar el 
*error de predicción esperado* o *error de prueba esperado*^[
$E_{rr}=E_\gamma[E_{X,Y}[L(Y,\hat{F}(X)) \mid \gamma]]$
]
$$E[L(Y,\hat{f}(X))] = E_{rr} = E[E_{rr\gamma}]$$
y usarlo para comparar y seleccionar modelos.
Este valor es más amable al análisis estadístico además
de que puede ser estimado con sólo el uso de la muestra
de entrenamiento.

<!-- El error de predicción esperado puede ser
estimado con la muestra de entrenamiento con el uso
de validación cruzada. La muestra de prueba se usa para estimar
el error de predicción (el que está condicionado a gamma)-->

El error de predicción esperado puede ser aproximado analíticamente
por métodos *in-sample* (AIC, DIC, MDL o SRR)^[En 
realidad, algunos de estos métodos estiman $E_y[E_{inn}]$. Para más 
información ver la sección 7.4 de @friedman2001elements .] 
o por métodos de reuso de muestras (validación cruzada, *bootstrap*).


Si tenemos muchos datos, lo mejor que podemos hacer es dividir
la muestra observada en 3 conjuntos ajenos:

- **Conjunto de entrenamiento**: se usa para entrenar el modelo 
(puede sobre-ajustar). Se recomienda, empíricamente $50\%$ 
de la muestra, dependiendo de su tamaño. 

- **Conjunto de prueba**: se usa para seleccionar modelos 
(aún si estima $E_{rr\gamma}$, se puede sobre-ajustar). 
Se recomienda, empíricamente, $25\%$ de
la muestra.

- **Conjunto de validación**: se utiliza para aproximar $E_{rr\gamma}$
de nuestro modelo final. Se recomienda, empíricamente, $25\%$ de
la muestra y sólo debe ser utilizado al final para calcular el error. 
**No** debe formar parte del proceso de selección de modelos (podría 
sobre-ajustar y estaría dando una sub-estimación del error de predicción real).


Si tenemos un menor tamaño de muestra, ésta se separa sólo en
conjuntos ajenos de entrenamiento ($80\%$) y validación ($20\%$). 
Con el conjunto de entrenamiento estimamos $E_{rr}$ (por medio de métodos de
reuso de muestra o *in-sample*) y comparamos / seleccionamos modelos.
Con el conjunto de validación estimamos $E_{rr\gamma}$ del modelo final. 


## Descomposición en varianza-sesgo

Si asumimos que estamos en el caso de regresión 
($Y = F(X) + \epsilon$, $E[\epsilon]=0$, $Var(\epsilon)= \sigma_\epsilon^2$)
y usamos la pérdida cuadrática se puede demostrar que ^[Por simplicidad,
supondremos que sólo nos interesa hacer predicción para $X = x_0$]
$$E_{rr}(x_0) = \sigma_\epsilon^2 + sesgo_\gamma^2(x_0) + Var_\gamma(x_0)$$

donde

- $\sigma_\epsilon^2 = Var(Y \mid X=x_0)$ es el error irreducible. 
No depende del ajuste, es aleatoriedad del fenómeno.

- $sesgo_\gamma^2(x_0) = (E[Y \mid X = x_0] -E_\gamma[\hat{f}_\gamma(x_0)])^2$ 
es el sesgo del método. Mide en promedio cuanto se desvía 
$\hat{f}_\gamma(x_o)$ del óptimo $E[y \mid X = x_0]$. Entre menos complejo sea
el método aumenta el sesgo y, por lo tanto, aumenta el error.

- $Var_\gamma(x_0) = E_\gamma[(\hat{f}_\gamma(x_0)-E_\gamma[\hat{f}_\gamma(x_0)])^2]$
es la varianza del método. mide qué tanto varían las predicciones
entre muestras de entrenamiento. Un método muy complejo es
un método inestable en el que las predicciones varían mucho
entres muestras de entrenamiento. 

Esta descomposición se encuentra, generalmente, independientemente 
de la función de pérdida y del tipo de problema. Por lo general,
$E_{rr\gamma}$ tiene un comportamiento con la misma estructura que 
$E_{rr}$ pero no igual.

Además, dicha descomposición nos muestra cómo, para minimizar el error de
predicción, es conveniente encontrar un equilibrio entre sesgo
y varianza: seleccionar un modelo suficientemente complejo para
que capture la estructura de los datos pero no tan complejo
para capturar ruido.


```{r , fig.cap = "Varianza-Sesgo: entrenamiento vs prueba.", cache = F}
 
## Método: k vecinos más cercanos
## Datos: Munich Rent Standards 
## Librerias: kknn

data(miete)

tamano <- floor(0.65 * nrow(miete))

set.seed(203040)

entrena.ind <- sample(seq_len(nrow(miete)), 
                      size = tamano)

entrena <- miete[entrena.ind, ]

prueba <- miete[-entrena.ind, ]


errores.vmc <- ldply(c(1, 5, 10, 20, 50, 100, 250, 500), 
                     function(i) {
  
    vecino.k.prueba <- kknn(nmqm ~ wfl + bjkat + zh, 
                            train = entrena, 
                            test = prueba, 
                            k = i, kernel = "rectangular")
    
    vecino.k.entrena <- kknn(nmqm ~ wfl + bjkat + zh, 
                             train = entrena, 
                             test = entrena, 
                             k = i, kernel = "rectangular")
    
    error.prueba <- mean((fitted(vecino.k.prueba) - 
                            prueba$nmqm)^2)
    error.entrena <- mean((fitted(vecino.k.entrena) - 
                             entrena$nmqm)^2)
    
    data.frame(k = i, prueba = error.prueba, 
               entrenamiento = error.entrena)
})

err.m <- errores.vmc %>% 
  gather(variable, valor, -k) %>% 
  mutate(n.k = floor(500/k) )
  
err.m %>% 
  ggplot(aes(x = factor(n.k), y = valor, 
             colour = variable,
             group = variable)) + 
  geom_line() + 
  geom_point() + 
  xlab("Grados de libertad") + 
  ylab("Error")
```

# Árboles de clasificación y regresión

Los modelos basados en árboles son herramientas poderosas
para la predicción. Estos van particionando, recursivamente, 
el espacio de covariables (*feature space*) en rectángulos 
por medio de modelos simples, como constantes. En esta 
sección, nos enfocaremos en el método *CART*^[@breiman1984classification]. 
Aunque este tipo de árboles, por si solos, son fácilmente superados 
en la predicción, consituyen la base teórica de técnicas muy poderosas 
como bosques aleatorios.

Para simplificar, aunque posteriormente veremos que en realidad
este enfoque es el más conveniente, CART hace 
particiones binarias. Primero, se parte el espacio en dos regiones
seleccionando la covariable y el corte que ajuste mejor; después,
una o ambas de estas regiones se particionan en otras dos de la misma
manera y así sucesivamente. 

Esta forma de particionar el espacio nos permite representar el modelo
gráficamente por medio de un árbol binario, lo que lo hace un modelo fácil
de interpretar: los nodos intermedios pueden entenderse como decisiones que
dependen de las covaraibles del modelo, los nodos terminales representan 
la partición final obtenida. En cada uno de los nodos terminales se elige, 
según algún criterio, un número o clase de predicción. Cómo se puede ver 
en los árboles binarios, este modelo permite capturar interacciones. 


```{r , fig.cap=sprintf("Árbol de clasificación. Representación %s.", c("espacial", "en árbol binario")), cache = F}

## Método: CART
## Datos: flores de Fisher
## Librerias: tree, rpart, ggdendro
 
## Modelos
control.completo <- rpart.control(cp=0, minsplit=10,
                                minbucket=1, xval=10, maxdepth=30)

arbol1 <- tree(Species ~ Sepal.Width + Petal.Width, data = iris)
arbol2 <- rpart(Species ~ Sepal.Width + Petal.Width, method = "class", 
               data = iris, control = control.completo)

## Particion del feature space
plot(iris$Petal.Width, iris$Sepal.Width, pch=19,col=as.numeric(iris$Species))
partition.tree(arbol1, label="Species", add = T)
legend(1.75,4.5,legend=unique(iris$Species),col=unique(as.numeric(iris$Species)),pch=19)

## Dendograma
ddata <- dendro_data(arbol2, uniform =T)

ggplot() + 
  geom_segment(data = ddata$segments, 
               aes(x = x, y = y, xend = xend, yend = yend), colour = "brown", size = 1) + 
  geom_text(data = ddata$labels, 
            aes(x = x, y = y, label = label), colour = "#006633", size = 3, vjust = -1) +
  geom_text(data = ddata$leaf_labels, 
            aes(x = x, y = y, label = label), colour = "#006633", size = 4, vjust = 1) +
  theme_dendro() 

```

Ahora queda por responder las siguientes preguntas: ¿Cómo elegir las 
particiones? ¿Cómo predecir en los nodos terminales? ¿Cómo declarar
un nodo terminal?

## Problema de regresión

Para este problema, supondremos que nuestros datos consisten 
de $p$ covariables, una variable respuesta cualitativa y $N$ 
observaciones, es decir, $(x_i,y_i)$ para $i=1, \ldots, N$ con 
$x_i = (x_{i1}, \ldots, x_{ip})$. 

Para construir nuestro árbol, recursivamente iremos dividiendo
cada nodo que se vaya generando con ayuda de las covariables del modelo 
$(X_1, \ldots, X_p)$. Se escogerá entre  particiones tales que

$$R_1(j,z) = \{X \mid X_j \leq z\} \quad \text{y} \quad R_2(j,z) = \{X \mid X_j > z\} \quad \text{si $X_j$ es continua}$$

$$R_1(j,s) = \{X \mid X_j \in s\} \quad \text{y} \quad R_2(j,s) = \{X \mid X_j \in s^c\} \quad \text{si $X_j$ es categórica}$$


Ahora bien, uno podría pensar que nuestro objetivo es dividir nuestro
espacio de covariables en las  regiones $R_1, \ldots, R_M$^[nodos terminales] 
tal que minimicen óptimamente 
$$\sum\limits_{i=1}^N (y_i - \sum\limits_{m=1}^M c_mI(x_i\in R_m))^2$$
con $c_m = promedio(y_i \mid x_i \in R_m)$. Sin embargo, este problema
no es computacionalmente factible. Lo que haremos es usar un algoritmo
codicioso para generar nuestro árbol, en otras palabras, las particiones
se escogerán de manera miope o local intentado separar las clases de 
un nodo lo mejor que se pueda.

Lo que se hace es, para cada nodo que vaya generando nuestro árbol,
encontrar la variable $j$ y el valor $z$ tal que^[Para simplificar la notación, supondremos que todas nuestras covariables son continuas.] 

$$\underset{j,z}{\text{min}}  \sum_{x_i \in R_1(j,z)}(y_i-c_1)^2 + \sum_{x_i \in R_2(j,z)}(y_i-c_2)^2 $$

Este proceso se repite hasta construir el árbol más grande que podamos 
tener.^[Mediante este algoritmo, no obtenemos la respuesta óptima pues
necesitaríamos "equivocarnos" para encontrarla.] Después nos encargaremos 
de *podarlo* para tener un mejor árbol predictor.

Se define la *impureza* de un nodo $t$ de un árbol $T$, para el problema 
de regresión, como $$i(t) = \frac{1}{N_t}\sum_{x_i \in R_t}(y_i-c_t)^2$$  y la
*impureza del árbol* como $$i(T) = \sum_{t \in T}q(t)i(t)$$ con $q(t) = N_t$.
Se puede ver que nuestro algoritmo miope se puede traducir a 
$$\underset{t -> t_1\cup t_2}{\text{min}} N_{t_1}i(t_1) + N_{t_2}i(t_2)$$
para todos los nodos que se vayan generando con la finalidad de disminuir 
la impureza del árbol lo más que se pueda. 

Una vez construido el árbol, para un valor $x_i$, $x_i \in R_m$
se predice el valor $c_m$.

## Problema de clasificación

Para el problema de clasificación, se pueden utilizar varias 
medidas de impureza. Estás usan las proporciones de casos 
en un nodo $t$ que caen en cada categoría $(p_1(t), \ldots, p_K(t))$ 
para dar una cuantificación de su impureza. 

Las más populares son las siguientes:

- **Entropía**: mide la revoltura o desorden de las clases,
es parecida a la devianza.^[La teoría estipula logaritmo base 2,
sin embargo, se puede usar cualquier base.]

$$i(t) = - \sum\limits_{k=1}^Kp_k(t)log(p_k(t))$$

- **Índice de Gini**: mide la "varianza" de las categorías,
es suma de varianzas bernoulli (multinomial).

$$i(t) = \sum\limits_{k=1}^Kp_k(t)(1-p_k(t)) = 1 -\sum\limits_{k=1}^Kp_k(t)^2$$

- **Error de clasificación**: calcula el porcentaje
de casos que mal clasificados con respecto a la categoría
más frecuente en el nodo.

$$i(t) = 1 - \underset{j}{\text{max}}(p_j(t))$$

Cuando $K=2$ estas 3 medidas se comportan similarmente.
Sin embargo, cuando $K > 2$ se recomienda usar, para
crecer el árbol, Gini o Entropía debido a que contemplan
lo que pasa a lo largo de todas las clases, no sólo la
clase dominante.^[Por ejemplo si tenemos los nodos $(500,200,200)$
y $(500,50,350)$ el error de clasificación es el mismo
pero el segundo nodo es mejor porque separa mejor las clases.]

```{r , fig.cap = "Comparación de medidas de impureza. K = 2."}
 
gini <- function(p){
  f <- 1-p^2 -(1-p)^2
  return(f)
}

entropia <- function(p){
  f <- -p*log2(p) - (1-p)*log2(1-p)
  f <- f/2
  return(f)
  }

clasificacion <- function(p){
  f <- 1-pmax(p,(1-p))
  return(f)
}

datos <- data.frame(x = seq(0.001, .999, length =300 )) %>%  
  mutate(gini = gini(x), entropia_escalada = entropia(x), 
         clasificacion = clasificacion(x)) %>% 
  gather(variable, valor, -x)

datos %>% ggplot(aes(x = x, y = valor, group = variable, color = variable)) +
  geom_line() + 
  ylab("y")
```

Para el caso de clasificación, la impureza del árbol 
está dada por 

$$i(T) = \sum_{t \in T}q(t)i(t)$$ con $q(t) = \frac{N_t}{N}$.

y nos interesa resolver en todos los nodos^[Se usa $\frac{N_{t_1}}{N_t}$ y $\frac{N_{t_1}}{N_t}$ para
que sea un problema local.] 
$$\underset{t -> t_1\cup t_2}{\text{min}} \frac{N_{t_1}}{N_t}i(t_1) + \frac{N_{t_2}}{N_t}i(t_2)$$
 

Una vez construido el árbol, para un valor $x_i$, $x_i \in R_m$
se predice con la categoría más frecuente en el nodo.

## Costo-Complejidad

Como se mencionó en la sección anterior, como primer paso para
construir nuestro árbol final, el que utilizaremos para predecir,
construiremos el árbol más grande posible $T_{max}$. Sin embargo, 
este árbol va a tener varianza alta debido a que es un modelo muy 
complejo y está sobre-ajustando, lo que nos llevaría a un mayor error de 
predicción.

Lo que se puede hacer es pensar en un criterio de paro para el 
tamaño del árbol. Una primera opción podría ser dividir un 
nodo sólo si el decrecimiento en la impureza del árbol excede
alguna cuota, pero con este criterio podríamos perder buenas 
separaciones debajo de las separaciones malas.

Una mejor estrategia es podar el árbol (*prunning*) mediante
el *costo-complejidad*. Este consiste en sumarle a la impureza
del árbol o el error de clasificación (dependiendo el caso) una
penalización por el tamaño del árbol $\mid T \mid$ multiplicado
por una constante fija $\alpha$, es decir,

$$c_\alpha(T) = i(T) + \alpha\mid T \mid$$

En un principio uno podría pensar en encontrar un árbol óptimo
$T^*(\alpha)$ que minimice $c_\alpha(T)$ para toda $\alpha \geq 0$^[
$\alpha$ más grande penaliza más fuerte el tamaño del árbol y lleva a 
soluciones con árboles más chicos.] y hacer validación cruzada 
para elegir el mejor parámetro, pero este es un problema demasiado 
grande. 

Para resolver este problema, se puede demostrar que para cada
$\alpha$ existe un único árbol que minimiza el error de 
costo-complejidad. Además, vale la pena observar que todas soluciones
$T^*(\alpha)$ son sub-árboles del árbol más grande $T_{max}$ y que, a pesar
de que existen un número infinito de $\alpha \geq 0$ sólo existe
un número finito de sub-árboles. 

@breiman1984classification demuestra que existe una sucesión anidada
de árboles  $T_{max} \leq T_1 \leq T_2 \leq \ldots \leq T_{k+1}$  y una
succión correspondiente $\alpha_0 \leq \alpha_1 \leq \ldots \leq \alpha_k$
tal que $T^*(\alpha) = T_j$ si $\alpha_{j-1} \leq \alpha < \alpha_j$, es decir,
no necesitamos explorar todas las combinaciones posibles de cortes del
árbol más grande para encontrar una solución para una determinada $\alpha$ y, 
además, no necesitamos encontrar el comportamiento de los árboles para todas 
las  $\alpha \geq 0$, sólo nos basta encontrar el árbol óptimo para un número
finito de $\alpha$ y estos árboles son sub-árboles del árbol óptimo generado
por $\alpha$ más chica. 

Una ves hecho esto encontramos cuál de estos árboles nos da el menor 
error de predicción o de validación cruzada y ese lo elegimos como 
el árbol final.

Como nota final, el costo complejidad para problemas de clasificación
está dado por 
$$c_\alpha(T) = \bar{err} + \alpha\mid T \mid$$ 
Aunque también se puede usar la impureza del árbol es más
común que se use el error de clasificación calculado con
la muestra de entrenamiento. 

## Ventajas 

- Son fáciles de interpretar.

- Capturan interacciones entre las covariables.

- No es necesario transformar variables.

- Son robustos a valores atípicos ya que pueden crear un nodo 
exclusivo para ese valor y las decisiones de clasificación 
son muy generales.

- Son robustos ante valores faltantes pues 1) pueden considerar
el valor faltante, si la covariable es categórica, con una categoría
más y ver que los individuos con valores faltantes se comportan
distinto a los que si respondieron y 2) crean cortes sucedáneos
buscando variables de entrada que más asemejen el corte encontrado.

- Se ajustan rápidamente.

- Usan cortes binarios para evitar que se acaben los datos en los nodos
muy rápido. además de que cortes múltiples pueden ser recreados por
una serie de varios cortes binarios.

- Se pueden usar combinaciones lineales para separar un nodo, 
pero se pierde interpretabilidad.

## Desventajas

- Difícilmente capturan estructuras lineales (regresión lineal).

- En la interpretación, algunas covariables pueden enmascarar a otras.
Una variable que no está en el árbol no quiere decir que no sea buena 
(cortes sucedáneos, qué tan cerca estuvieron de ganar).

- Son muy inestables ($Var_\gamma(T)$ es muy alta). Debido a que son 
modelos jerárquicos, es muy fácil que con un pequeño cambio en la muestra 
cambie radicalmente el árbol. Esto produce un desempeño predictor
relativamente malo (para este punto, también conviene ver los cortes
sucedáneos).

- Cuando tienen como covariable una variable categórica con muchas 
categorías el árbol sobre-ajusta muy rápido. Esto se debe a lo
exhaustivo de la búsqueda (como se construyen los cortes de variables
categóricas, ver @breiman1984classification o @friedman2001elements )

## Ejemplo

```{r , fig.cap = c("Árbol completo", "Error de validación cruzada (x-val)", "Error de prueba vs error de entrenamiento", "Árbol podado")}

## Método: CART (regresión)
## Datos: Boston Housing (librería mlbench):
## Housing data for 506 census tracts of Boston from the 1970 census
## medv =	median value of owner-occupied homes in USD 1000's
## Librerias: rpart, ggdendro
 
data(BostonHousing)
set.seed(15)
train <- sample(1:nrow(BostonHousing),400)
BostonHousing$train <- F
BostonHousing$train[train] <- T
entrena.arb <- BostonHousing[BostonHousing$train,]
prueba.arb <- BostonHousing[!BostonHousing$train,]
entrena.arb$train <- NULL

## Modelo completo
modelo.arb.completo <- rpart(medv~., data = entrena.arb, method = "anova", 
                       control=rpart.control(cp=0 , xval=10, minbucket=1))

## Dendograma completo
ddata <- dendro_data(modelo.arb.completo , uniform =T)

ggplot() + 
  geom_segment(data = ddata$segments, 
               aes(x = x, y = y, xend = xend, yend = yend), colour = "brown", size = 1) + 
  geom_text(data = ddata$labels, 
            aes(x = x, y = y, label = label), colour = "#006633", size = 1, vjust = -1) +
  geom_text(data = ddata$leaf_labels, 
            aes(x = x, y = y, label = label), colour = "#006633", size = 1, vjust = 1) +
  theme_dendro() + scale_y_reverse(expand = c(0.2, 0)) + coord_polar(theta="x")

## Validación cruzada: cp = 15
plotcp(modelo.arb.completo)

error.completo <- modelo.arb.completo$cptable %>%
  data.frame() %>% 
  dplyr::select(nsplit, CP, xerror) 

error.completo %>% head(40)

alpha <- error.completo$CP

## Prueba vs entrenamiento
errores.vmc <- ldply(alpha, 
                     function(i) {
                       
                      modelo <- rpart(medv~., data = entrena.arb, method = "anova", 
                                                             control=rpart.control(cp=i , xval=10, minbucket=1))
                      
                      error.entrena <- mean((predict(modelo) - entrena.arb$medv)^2)
                       
                      error.prueba <- mean((predict(modelo, prueba.arb)- prueba.arb$medv)^2)
                       
                       data.frame(CP = as.numeric(i), prueba = error.prueba, 
                                  entrenamiento = error.entrena)
                     })

errores.vmc %>% 
  left_join(error.completo %>% 
              dplyr::select(-xerror)) %>% 
  gather(variable, valor, -CP, -nsplit) %>% 
  ggplot(aes(x = nsplit, y = valor, 
             colour = variable,
             group = variable)) + 
  geom_line() +
  geom_vline(xintercept = 19) +
  xlab("Altura del árbol") + 
  ylab("Error")

## Modelo podado

modelo.arb.final <- rpart(medv~., data = entrena.arb, method = "anova", 
                             control=rpart.control(cp=.0035 , xval=10, minbucket=1))

## Dendograma podado

ddata <- dendro_data(modelo.arb.final , uniform =T)

ggplot() + 
  geom_segment(data = ddata$segments, 
               aes(x = x, y = y, xend = xend, yend = yend), colour = "brown", size = 1) + 
  geom_text(data = ddata$labels, 
            aes(x = x, y = y, label = label), colour = "#006633", size = 3, vjust = -1) +
  geom_text(data = ddata$leaf_labels, 
            aes(x = x, y = y, label = label), colour = "#006633", size = 3, vjust = 1) +
  theme_dendro() 

## error de prueba

mean((predict(modelo.arb.final, prueba.arb)- prueba.arb$medv)^2)
```


<!---------------------------------------------------------------------------->
<!------------------------ Empiezan siguientes temas ------------------------->
<!---------------------------------------------------------------------------->

# Bagging y Bosques Aleatorios

Como vimos, los árboles de clasificación sufren de varianza inherente al
modelo alta aunque sesgo bajo^[Recordemos: $E_{rr}(x_0) = \sigma_\epsilon^2
+ sesgo_\gamma^2(x_0) + Var_\gamma(x_0)$]. La idea de los métodos que
veremos a continuación es aumentar el sesgo *suavizando* la predicción al
promediar la predicción de distintos árboles.

## Bagging

Intuitivamente, **bagging** es considerar la *sabiduría de las masas*. En
lugar de estimar de un sólo árbol, ponderamos la opinión de muchos.

Consideremos $L^1, L^2, ..., L^B$ muestras **independientes** e idénticamente
distribuidas del mismo fenómeno $F(x)$.^[Por simplicidad, suponemos que el
fenómeno tiene una respuesta cuantitativa.] Si con cada una obtenemos
un árbol $T_{L^i}$, definimos
$$T'(x) = \frac{1}{B} \sum_{i=1}^B T_{L^i}(x)$$
como el *árbol bagging ideal* de manera que la predicción de $T'(x)$ es el
promedio de los árboles $T_{L^i}(x)$. La pregunta es, ¿cómo se desempeña $T'$?

Con respecto al sesgo podemos ver que
$$\begin{aligned}
  E[T'(x)] &= \frac{1}{B} \sum_{i=1}^B T_{L^i}(x) \\
    &= \frac{1}{B} \sum_{i=1}^B T_{L}(x) = E[T_L(x)]
  \end{aligned}$$
Por lo que el sesgo de $T'$ es idéntico al de un único árbol.

Sin embargo, la varianza cambia de la siguiente manera
$$Var(T'(x)) = \frac{1}{B} Var(T_L(x))$$
a manera de que si $B$ es grande, el error cuadrático medio de $T'$ puede ser
mucho menor que el de un árbol individual.

La parte relevante aquí es la palabra **independiente** de las muestras. Debido
a que rara vez contamos con más de una muestra y difícilmente independientes
entre sí, podemos utilizar muestras *bootstrap*.^[Intuitivamente, si $F(x)$ es
el mundo real y hacemos inferencia de él con una muestra $L$; *bootstrap*
supone $L$ como el mundo real y hace inferencia de él con submuestras
$L^{*i}$.]

Sea $L = \{(x^i, y^i)_{i=1}^N\}$ una muestra de entrenamiento y sean
$L^{*1}, L^{*2}, ..., L^{*B}$ muestras *bootstrap* de $L$. Es decir, cada
$L^{*i}$ es una muestra con reemplazo de $(x^i, y^i) \in L$ con mismo tamaño de
muestra $N$. Entonces el estimador *bagging por bootstrap* viene dado por:
$$\hat{T}^*(x) = \frac{1}{B} \sum_{i=1}^B T^{*i}(x)$$
para variable de respuesta cuantitativa. En el caso cualitativo:
$$\hat{T}^*(x) = argmax_g \{\sum_{i=1}^B I(T^{*i}(x) = g)\}$$

Por propiedades de *bootstrap* se puede ver que el sesgo de $T^*$ se mantiene.
Para ver que pasa con el error cuadrático medio, se define $T^\dagger$ como
el estimador *bootstrap* ideal que extrae muestras directamente de $F(x)$ tal
que $T^\dagger(x) = E[T_L(x)]$. Entonces:
$$\begin{aligned}
  E[F(x) - T_L(x)]^2 &= E[F(x) - T^\dagger(x) + T^\dagger(x) -
      T_L(x)]^2 \\
    &= E[F(x) - T^\dagger(x)]^2 + E[T_L(x) - T^\dagger(x)]^2 \\
    &\geq E[F(x) - T^\dagger(x)]^2
\end{aligned}$$
Por lo tanto, vemos que al predecir con *bagging* tomando muestras de la
población, el error cuadrático medio queda exactamente igual o mejora^[Ojo:
Esta afirmación no se mantiene para la pérdida 0-1. En este caso, *bagging* de
malos estimadores puede empeorar la predicción.]. En la práctica no se puede
muestrear de la población original, por lo que esto sólo sugiere que
*bootstrap* puede ayudar, pero no hay garantía.

Por esto, se busca la manera de *decorrelacionar* las muestras para mejorar el
error cuadrático. Esto da pie a la técnica que veremos a continuación.

## Bosques Aleatorios

Como vimos, *bagging* puede mejorar la predicción.^[En la práctica
generalmente se tiene una mejora.] Sin embargo, no está garantizado pues las
muestras *bootstrap* están correlacionadas entre sí.

La pregunta es, ¿cómo afecta esta correlación?

Supongamos que tenemos $T^*(x)$ un árbol ajustado con *bagging* obtenido de
muestras *bootstrap* de $F(x)$. Entonces, si condicionamos a la muestra de
entrenamiento $L$, vemos que:
$$Var(T^*(x)) = E[Var(T^*(x)|L)] + Var(E[T^*(x)|L])$$
donde el primer término es la variación inducida por el remuestreo y el segundo
es la varianza de cada submuestra.

Se puede ver que si
$$T(x) = \frac{1}{B} \sum_{i=1}^B T^{*i}(x)$$
es un modelo basado en árboles obtenidos de una muestra *bootstrap*,
entonces^[Esto porque
$\begin{aligned}
  Var(E[T^*(x)|L]) &= E_L[(E[T^*(x)|L])^2] - (E[T^*(x)])^2 \\
    &= E_L[E[T^{*i}(x)|L]E[T^{*j}(x)|L]] - (E[T^*(x)])^2 \\
    &= E_L[E[T^{*i}(x)T^{*j}(x)|L]] - (E_L[E[T^*(x)|L]])^2 \\
    &= Cov(T^{*i}(x), T^{*j}(x))
\end{aligned}$]
$$\begin{aligned}
  Var(T(x)) &= \frac{1}{B} E[Var(T^*(x)|L)] + Var(E[T^*(x)|L]) \\
    &= \frac{1}{B}E[Var(T^*(x)|L)] + Cov(T^{*i}(x), T^{*j}(x))
\end{aligned}$$

Por lo que el error cuadrático medio decrece siempre que logremos
*decorrelacionar* la manera como se construyen los árboles que conforman
$T(x)$.

Esto lo logramos con **bosques aleatorios** de la siguiente forma:

1. Para $b = 1, 2, ..., B$ y $m \leq p$ fija^[$p$ es el número de variables
en la muestra $L$]
    - Seleccionar muestra *bootstrap* $L_b^*$ de $L$
    - Construir un árbol $T_b^*$ basado en $L_b^*$. Pero en cada nodo,
  consideramos al azar $m$ variables de las $p$ disponibles como candidatas
  para cortes.
2. El predictor está dado por
$$T_{b.a.}(x) = \frac{i}{B} \sum_{b=1}^B T^{*b}(x)$$
para regresión. Para clasificación por
$$T_{b.a.}(x) = argmax_g \{ \sum_{b=1}^B I(T^{*b}(x) = g) \}$$

Por lo tanto, podemos pensar en **bosques aleatorios** como *bagging* con
árboles *decorrelacionados*. Notemos que los parámetros para controlar la
complejidad de los modelos vienen dados por $B$ el número de árboles y $m$ el
número de variables candidatas a corte.

## Bosques Aleatorios en la práctica

A continuación algunas sugerencias de como buscar los valores de los parámetros
de complejidad y algunas reglas de dedo.^[@friedman2001elements]

**$B$: Número de árboles**

Se elige minimizando una estimación honesta del error. Esto implica hacer $B$
muy grande hasta que dicha estimación se estabilice.

**$m$: Número de variables candidatas para corte**

La regla de dedo del autor (y del paquete de R) sugiere:

  - $m = \lfloor p / 3 \rfloor$ para regresión
  - $m = \lfloor \sqrt{p} \rfloor$ para clasificación

**Tamaño de árboles**

Dado que queremos incrementar la varianza de los árboles independientes, se
recomienda considerar árboles sin podar. Este parámetro también se puede
afinar.

## Estimación *Out-Of-Bag* (OOB) del error

El proceso de muestreo de los *bosques aleatorios* da una manera simple de
construir una estimación honesta del error de predicción.

Sea $z_i = (x_i, y_i) \in L$. Definimos
$$T_{z_i} = \frac{1}{B_{z_i}} \sum_{z_i \notin L^{*j}} T^{*j}(x_i)$$
donde $B_{z_i} = \sum_j I(z_i \notin L^{*j})$. Es decir, construimos el
predictor promediando sólo aquellos árboles que **no** usaron $z_i$ en su
construcción.

Por lo tanto, la estimación *OOB* del error de prueba para el bosque es:^[Es
importante recalcar que si $B$ es grande, $\hat{Err_{oob}}$ es similar
a la estimación por validación cruzada  $N$. Sin necesidad de ajustar el modelo
$N$ veces.]
$$\hat{Err_{oob}} = \frac{1}{N} \sum_{i=1}^N(y_i - T_{z_i})^2$$

## Importancia de las variables

Otra bondad de los *bosques aleatorios* es que nos permiten calcular una
medida de la importancia predictiva de las variables de entrada.

Para calcular dicha medida, consideremos un árbol $T^{*j}$. Usando la muestra
*OOB* de $T^{*j}$, que es $L - L^{*j}$, estimamos su error *OOB*:
$$\hat{Err_{oob}} = \frac{1}{A_j} \sum_{z_i \in L - L^{*j}}
(y_i - T^{*j}(x_i)^2)$$
donde $z_i = (x_i, y_i)$ y $A_i = |L - L^{*j}|$ (el tamaño de muestra *OOB*).

Ahora, permutamos los valores de la variable $X_j$ en la muestra *OOB* para
hacer algo similar a un análisis de sensibilidad y calculamos
$\hat{Err_{oob}}(T^{*j})$ con la muestra *OOB* con la $k$-ésima variable
permutada.

Finalmente, promediamos sobre el bosque
$$\hat{Imp}(k) = \frac{1}{B} \sum_{j=1}^B (\hat{Err_k(T^{*j})} -\hat{Err_{oob}}(T^{*j}))$$
y denotamos esta medida como la *importancia relativa de la variable
$x_j$*.^[Podemos interpretar la importancia como el decremento en precisión que
resulta de *quitar* la variable $x_j$.]


```{r , fig.cap = c("Error Out-Of-Bag para selección de parámetros", "Importancia de las Variables")}

## Método: RandomForest
## Datos: Boston Housing (librería mlbench)
## Librerías: randomForest

b <- 2000

## Probamos con distintos parámetros
bosque.2 <- entrena.arb %>%
  randomForest(medv ~ ., data = ., ntree = b, mtry = 2)
bosque.5 <- entrena.arb %>%
  randomForest(medv ~ ., data = ., ntree = b, mtry = 5)
bosque.13 <- entrena.arb %>%
  randomForest(medv ~ ., data = ., ntree = b, mtry = 13)

data.table(iteracion = 1:b,
    mtry2 = bosque.2$mse,
    mtry5 = bosque.5$mse,
    mtry13 = bosque.13$mse) %>%
  gather(variables, error, -iteracion) %>%
  ggplot(aes(x = iteracion, y = error, colour = variables,
      group = variables)) +
    geom_line()

## Nos quedamos con los mejores parámetros
bosque <- entrena.arb %>%
  randomForest(medv ~ ., data = ., ntree = 1000, do.trace = 100, mtry = 13)

varImpPlot(bosque)

## Error cuadrático medio
## Recordemos que el de árboles de decisión era de ~ 17
mean((predict(bosque, prueba.arb) - prueba.arb$medv)^2)


```


## Adaboost

Este método también se basa en un ensamble de árboles como *bosques
aleatorios* y *bagging*. La diferencia con estos otros métodos mencionados es
que la sucesión de árboles se "adapta" a lo largo de las iteraciones.^[Otro
enfoque de ver a *Adaboost*, es que es un caso particular de la familia de
modelos aditivos. Esta familia de modelos busca encontrar un conjunto ortogonal
de funcionales en cada iteración. Aunque esto queda fuera del alcance de este
curso, el lector interesado puede ver @friedman2001elements]

Supongamos que la variable de respuesta es $Y = \{-1, 1 \}$ y $X$ las
variables de entrada. Construiremos una sucesión de $T_1, T_2, ..., T_M$
árboles para versiones **ponderadas** de la muestra de entrenamiento. El
clasificador agregado será

$$ T_a(x) = signo \{ \sum_{m = 1}^M \alpha_m T_m (x) \} $$

Los pesos $\alpha_m$ dependerán de la calidad de la predicción de $T_m (x)$.

Notemos que si los pesos $\alpha_m$ son iguales, tenemos un clasificador por
mayoría de votos, pues

$$\begin{aligned}
  T(x) &= signo \{ \alpha \sum_{m = 1}^M T_m (x) \} \\
    &= signo\{ T_m (x) \}
\end{aligned}$$

### Algoritmo de Adaboost

1. Inicializamos los pesos $\omega_i = \frac{1}{N}$ con $i = 1, 2, ..., N$

2. Para $m = 1, 2, ..., M$:

  a) Ajustar $T_m(x)$ a $L$ ponderado por $\omega_i$'s
  b) Calcular tasa de error ponderada de $T_m(x)$ como
    $$ \hat{err_m} = \frac{\sum_{i=1}^N \omega_i^{(m)} I(y_i \neq T_m(x_i))
    }{\sum_{i=1}^N \omega_i^{(m)}}$$
      y hacer $\alpha_m = log(\frac{1 - \hat{err_m}}{\hat{err_m}})$
  c) Poner
    $$ \omega_i^{(m+1)} = \omega_i^{(m)} exp(\alpha_m I(y_i \neq T_m(x_i))) $$

3. Clasificar con
$$ T_a(x) = signo \{ \sum_{m = 1}^M \alpha_m T_m (x) \} $$

### Algunas consideraciones

**Consideración 1:** ¿Cómo ajustar $T_m(x)$ con la muestra $L$ ponderada por
$\omega_1, \omega_2, ..., \omega_N$?

Como vimos, para árboles tenemos distintos criterios de corte y decisión en
nodos terminales. Por ejemplo, si queremos ajustar un árbol con la muestra
ponderada y la **entropía** como medida de pureza, tenemos:

$$i(t) = -\sum_{k=1}^K p_k(t)log(p_k(t))$$
con $p_k(t) = \frac{\sum \omega_i I(g_i = k)}{\sum \omega_i}$

**Consideración 2:** ¿Cómo obtenemos dichos pesos $\omega_i$?

Supongamos $\omega_1^{(m)}, \omega_2^{(m)}, ..., \omega_N^{(m)}$ dados.^[
Recordemos que podemos inicializar $\omega_i^{(1)}$ con $\frac{1}{N}$ y tener
un estimador por mayoría de votos]
Entonces, estimamos $\omega_i^{(m+1)} = \omega_i^{(m)} k_i^{(m)}$, donde

$$
k_i^{(m)} = 
  \begin{cases}
    \frac{1-\hat{err_m}}{\hat{err_m}} & \quad \text{si } T_m(x) \neq y_i\\
    1 & \quad \text{si } T_m(x) = y_i
  \end{cases}
$$

Notemos que si $\hat{err_m} < 1/2$, entonces
$\frac{1-\hat{err_m}}{\hat{err_m}} > 1$. Es decir, si el predictor se
desempeña mejor que el azar pero se equivoca al predecir $(y_i, x_i)$,
entonces *aumentamos* el peso de dicha observación en la siguiente iteración.
Intuitivamente, estamos forzando a que el algoritmo *preste más atención* a
observaciones mal clasificadas.

En el caso en que $\hat{err_m} > 1/2$, tenemos que $\alpha_m < 0$ y entonces
*se voltea* la contribución de $T_m(x)$^[Recordemos que $\alpha_m =
log(\frac{1-\hat{err_m}}{\hat{err_m}})$].

**Consideración 3:** Es importante recalcar que si $\hat{err_m}$ se reduce,
$\alpha_m$ se incrementa y viceversa. Por lo que en cada iteración, además de
darle mayor importancia a los casos más difíciles de pronosticar, al predecir
le damos mayor peso a los árboles que mejor clasifican.


```{r}

## Método: Adaboost
## Librerías: rpart

set.seed(3003)

## Tamaño de muestra (de entrenamiento)
N <- 250

## Conjunto de enternamiento
entrena <- rdply(N, rnorm(10))
## Modelo 2 * I(x ^ 2 > 9) - 1
entrena$y <- 2*(apply(entrena[,-1]^2, 1, sum) > 9)-1

## Conjunto de prueba
prueba <- rdply(1000, rnorm(10))
prueba$y <- 2*(apply(prueba[,-1]^2, 1, sum) > 9)-1

## Árbol de un sólo corte para comparar
arbol.1 <- rpart(y~., data = entrena, maxdepth = 1, method = "class")

## Error de árbol con un corte:
mean(predict(arbol.1, newdata = prueba, type="class")!=prueba$y)

## Árbol completo
arbol <- rpart(y~., data = entrena, method = "class", cp = 0)
printcp(arbol)

## Árbol podado
arbol.pod <- prune(arbol, cp = 0.002)
## Error de árbol podado
mean(predict(arbol.pod, newdata = prueba, type = "class") != prueba$y)

## Ada Boost con árboles de 1 corte

arbol.lista <- list()
M <- 2000
alpha <- numeric(M)
error <- numeric(M)
error.prueba <- numeric(M)
error.entrena <- numeric(M)
w <- rep(1,nrow(entrena))
preds.boost <- numeric(N)
preds.boost.entrena <- numeric(N)

for(i in 1:M){
  #print(i)
  arbol.lista[[i]] <- rpart(y~., data = entrena,
      weights = w, maxdepth=1, method="class")
  preds <- predict(arbol.lista[[i]], newdata = entrena, type = "class")
  incorrectos <-  preds != entrena$y
  error[i] <- sum(incorrectos*w)/sum(w)
  alpha[i] <- log((1-error[i])/error[i])
  w <- w*exp(alpha[i]*(preds != entrena$y))
  # Función de predicción y evaluación
  preds.boost <- preds.boost + 0.5*alpha[i]*
      as.numeric(as.character(predict(arbol.lista[[i]],
          newdata = prueba, type="class")))
  preds.boost.entrena <- preds.boost.entrena + 0.5*alpha[i]*
              as.numeric(as.character(predict(arbol.lista[[i]],
                  newdata = entrena, type="class")))
  error.entrena[i] <- mean(sign(preds.boost.entrena) != entrena$y)
  error.prueba[i] <- mean(sign(preds.boost) != prueba$y)
}


## Graficamos error de entrenamiento
plot(error.entrena, type = "l", ylim = c(0, 0.6))
## Como referencia
abline(h = 0)
## Graficamos el error de prueba
lines(error.prueba, col = "red")
## Error con un árbol podado
abline(h = mean(predict(arbol.pod, newdata = prueba, type="class")!=prueba$y), col="blue")
## Error con un árbol
abline(h = mean(predict(arbol.1, newdata = prueba, type="class")!=prueba$y), col="green")

## Error de Adaboost
mean(sign(preds.boost) != prueba$y)

```